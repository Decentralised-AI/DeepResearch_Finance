from typing import List, Tuple
from deepsearcher.llm.base import BaseLLM
from deepsearcher.agent.collection_router import CollectionRouter
from deepsearcher.vector_db.base import RetrievalResult
from deepsearcher.tools import log

FOLLOWUP_QUERY_PROMPT = """You are using a search tool to answer the main query by iteratively searching the database. 
Given the following intermediate queries and answers, generate a new simple follow-up question that can help answer the 
main query. You may rephrase or decompose the main query when previous answers are not helpful. Ask simple follow-up 
questions only as the search tool may not understand complex questions.

## Previous intermediate queries and answers
{intermediate_context}

## Main query to answer
{query}

Respond with a simple follow-up question that will help answer the main query, do not explain yourself or 
output anything else.
"""


INTERMEDIATE_ANSWER_PROMPT = """Given the following documents, generate an appropriate answer for the query. 
DO NOT hallucinate any information, only use the provided documents to generate the answer. 
Respond "No relevant information found" if the documents do not contain useful information.

## Documents
{retrieved_documents}

## Query
{sub_query}

Respond with a concise answer only, do not explain yourself or output anything else.
"""


FINAL_ANSWER_PROMPT = """Given the following intermediate queries and answers, generate a final answer for the main query by combining relevant information. Note that intermediate answers are generated by an LLM and may not always be accurate.

## Documents
{retrieved_documents}

## Intermediate queries and answers
{intermediate_context}

## Main query
{query}

Respond with an appropriate answer only, do not explain yourself or output anything else.
"""

REFLECTION_PROMPT = """Given the following intermediate queries and answers, judge whether you have enough information 
to answer the main query. If you believe you have enough information, respond with "Yes", otherwise respond with "No".

## Intermediate queries and answers
{intermediate_context}

## Main query
{query}

Respond with "Yes" or "No" only, do not explain yourself or output anything else.
"""

GET_SUPPORTED_DOCS_PROMPT = """Given the following documents, select the ones that are support the Q-A pair.

## Documents
{retrieved_documents}

## Q-A Pair
### Question
{query}
### Answer
{answer}

Respond with a python list of indices of the selected documents.
"""

@describe_class(
    "This agent can decompose complex queries and gradually find the fact information of sub-queries. "
    "It is very suitable for handling concrete factual queries and multi-hop questions."
)
class ChainOfRAG:
    """
    Chain of RAG agent implementation. It is a multi-step RAG process where each step can refine
    the query and retrieval process based on previous results, creating a chain of increasingly
    focused and relevant information
    """

    def __init__(
            self,
            llm: BaseLLM,
            embedding_model,
            vector_db,
            max_iter: int = 4,
            early_stopping: bool = False,
            route_collection: bool = True,
            text_window_splitter: bool = True,
            **kwargs
    ):
        """
        Initialize the ChainofRAG agent with config params
        :param llm:
        :param embedding_model:
        :param vector_db:
        :param max_iter:
        :param early_stopping:
        :param route_collection:
        :param text_window_splitter:
        :param kwargs:
        """
        self.llm = llm
        self.embedding_model = embedding_model
        self.vector_db = vector_db
        self.max_iter = max_iter
        self.early_stopping = early_stopping
        self.route_collection = route_collection
        self.collection_router = CollectionRouter(
            llm=self.llm, vector_db=self.vector_db, dim=embedding_model.dimension
        )
        self.text_window_splitter = text_window_splitter

    def _reflect_get_subquery(self, query: str, intermediate_context: List[str]) -> Tuple[str, int]:
        chat_response = self.llm.chat(
            [
                {
                    "role": "user",
                    "content": FOLLOWUP_QUERY_PROMPT.format(
                        query=query,
                        intermediate_context="\n".join(intermediate_context)
                    )
                }
            ]
        )
        return chat_response.content, chat_response.total_tokens

    def _retrieve_and_answer(self, query: str) -> Tuple[str, List[RetrievalResult], int]:
        consume_tokens = 0
        if self.route_collection:
            selected_collections, n_token_route = self.collection_router.invoke(
                query=query, dim=self.embedding_model.dimension
            )
        else:
            selected_collections = self.collection_router.all_collections
            n_token_route = 0

        consume_tokens += n_token_route
        all_retrieved_results = []

    def retrieve(self, query: str, **kwargs) -> Tuple[List[RetrievalResult], int, dict]:
        """
        Retrieves relevant documents based on the input query and iteratively refines the search.
        The method iteratively refines the search query based on intermediate results, retrieves docs,
        and filters out supported documents. It keeps track of the intermediate contexts and token usage.

        :param query:
        :param kwargs:
        :return:
        """
        max_iter = kwargs.pop("max_iter", self.max_iter)
        intermediate_contexts = []
        all_retrieved_results = []
        token_usage = 0
        for iter in range(max_iter):
            log.color_print(f">> Iteration: {iter + 1}\n")
            # creates a follow-up query that helps fully answer the main query
            followup_query, n_token0 = self._reflect_get_subquery(query, intermediate_contexts)

            intermediate_answer, retrieved_results, n_token1 = self._retrieve_and_answer(followup_query)
            supported_retrieved_results, retrieved_results, n_token1 = self._get_supported_docs(
                retrieved_results, followup_query, intermediate_answer
            )
            all_retrieved_results.extend(supported_retrieved_results)








